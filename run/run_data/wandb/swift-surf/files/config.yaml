wandb_version: 1

_current_progress_remaining:
  desc: null
  value: 1.0
_custom_logger:
  desc: null
  value: 'False'
_episode_num:
  desc: null
  value: 0
_episode_storage:
  desc: null
  value: None
_last_episode_starts:
  desc: null
  value: '[ True]'
_last_obs:
  desc: null
  value: "OrderedDict([('achieved_goal', array([[ 4.8420656e-01, -5.1861488e-12, \
    \ 4.1103777e-01]], dtype=float32)), ('desired_goal', array([[-0.36585835,  0.51871973,\
    \  0.26197878]], dtype=float32)), ('observation', array([[ 4.8420656e-01, -5.1861488e-12,\
    \  4.1103777e-01,  0.0000000e+00,\n        -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\
    \ -3.0000001e-01,\n         0.0000000e+00, -2.2000000e+00,  0.0000000e+00,  2.0000000e+00,\n\
    \         7.8539819e-01,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n   \
    \      0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n      \
    \   2.8076324e-01,  6.2981808e-01,  2.8484035e-01,  4.9770206e-01,\n         2.8617457e-01,\
    \  5.5290097e-01,  3.1935012e-01,  6.6397059e-01,\n         3.5679859e-01,  2.0234461e-01,\
    \  4.6441135e-01,  1.8427321e-01,\n         5.5766946e-01,  2.7388373e-01,  5.7155538e-01,\
    \  5.0419664e-01,\n         6.2862748e-01,  5.5986226e-01]], dtype=float32))])"
_last_original_obs:
  desc: null
  value: None
_logger:
  desc: null
  value: <stable_baselines3.common.logger.Logger object at 0x0000021AA084A250>
_n_updates:
  desc: null
  value: 0
_num_timesteps_at_start:
  desc: null
  value: 0
_stats_window_size:
  desc: null
  value: 100
_total_timesteps:
  desc: null
  value: 300000
_vec_normalize_env:
  desc: null
  value: None
_wandb:
  desc: null
  value:
    cli_version: 0.14.0
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.11.1
    start_time: 1683527421.844862
    t:
      1:
      - 1
      - 55
      2:
      - 1
      - 55
      3:
      - 3
      - 15
      - 16
      - 22
      - 23
      - 35
      4: 3.11.1
      5: 0.14.0
      8:
      - 3
      - 5
action_limiter:
  desc: null
  value: clip
action_noise:
  desc: null
  value: None
action_space:
  desc: null
  value: Box(-1.0, 1.0, (7,), float32)
actor:
  desc: null
  value: "Actor(\n  (features_extractor): CombinedExtractor(\n    (extractors): ModuleDict(\n\
    \      (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n      (desired_goal):\
    \ Flatten(start_dim=1, end_dim=-1)\n      (observation): Flatten(start_dim=1,\
    \ end_dim=-1)\n    )\n  )\n  (latent_pi): Sequential(\n    (0): Linear(in_features=44,\
    \ out_features=400, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=400,\
    \ out_features=300, bias=True)\n    (3): ReLU()\n  )\n  (mu): Sequential(\n  \
    \  (0): Linear(in_features=300, out_features=7, bias=True)\n    (1): Hardtanh(min_val=-2.0,\
    \ max_val=2.0)\n  )\n)"
algo:
  desc: null
  value: TQC
algorithm:
  desc: null
  value: TQC
batch_norm_stats:
  desc: null
  value: '[]'
batch_norm_stats_target:
  desc: null
  value: '[]'
batch_size:
  desc: null
  value: 256
buffer_size:
  desc: null
  value: 300000
control_type:
  desc: null
  value: js
critic:
  desc: null
  value: "Critic(\n  (features_extractor): CombinedExtractor(\n    (extractors): ModuleDict(\n\
    \      (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n      (desired_goal):\
    \ Flatten(start_dim=1, end_dim=-1)\n      (observation): Flatten(start_dim=1,\
    \ end_dim=-1)\n    )\n  )\n  (qf0): Sequential(\n    (0): Linear(in_features=51,\
    \ out_features=400, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=400,\
    \ out_features=300, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=300,\
    \ out_features=25, bias=True)\n  )\n  (qf1): Sequential(\n    (0): Linear(in_features=51,\
    \ out_features=400, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=400,\
    \ out_features=300, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=300,\
    \ out_features=25, bias=True)\n  )\n)"
critic_target:
  desc: null
  value: "Critic(\n  (features_extractor): CombinedExtractor(\n    (extractors): ModuleDict(\n\
    \      (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n      (desired_goal):\
    \ Flatten(start_dim=1, end_dim=-1)\n      (observation): Flatten(start_dim=1,\
    \ end_dim=-1)\n    )\n  )\n  (qf0): Sequential(\n    (0): Linear(in_features=51,\
    \ out_features=400, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=400,\
    \ out_features=300, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=300,\
    \ out_features=25, bias=True)\n  )\n  (qf1): Sequential(\n    (0): Linear(in_features=51,\
    \ out_features=400, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=400,\
    \ out_features=300, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=300,\
    \ out_features=25, bias=True)\n  )\n)"
device:
  desc: null
  value: cpu
ent_coef:
  desc: null
  value: auto
ent_coef_optimizer:
  desc: null
  value: "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n\
    \    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach:\
    \ None\n    fused: None\n    lr: 0.00073\n    maximize: False\n    weight_decay:\
    \ 0\n)"
env:
  desc: null
  value: <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000021A7D1F8710>
env_name:
  desc: null
  value: PandaReachAO-v3
ep_info_buffer:
  desc: null
  value: deque([], maxlen=100)
ep_success_buffer:
  desc: null
  value: deque([], maxlen=100)
eval_freq:
  desc: null
  value: 5000
gamma:
  desc: null
  value: 0.98
goal_distance_threshold:
  desc: null
  value: 0.05
gradient_steps:
  desc: null
  value: 8
hyperparams:
  desc: null
  value:
    buffer_size: 300000
    ent_coef: auto
    gamma: 0.98
    gradient_steps: 8
    learning_rate: 0.00073
    policy_kwargs:
      log_std_init: -3
      net_arch:
      - 400
      - 300
    tau: 0.02
    train_freq: 8
    use_sde: true
joint_obstacle_observation:
  desc: null
  value: all2
learning_rate:
  desc: null
  value: 0.00073
learning_starts:
  desc: null
  value: 10000
limiter:
  desc: null
  value: sim
log_ent_coef:
  desc: null
  value: tensor([0.], requires_grad=True)
lr_schedule:
  desc: null
  value: <function constant_fn.<locals>.func at 0x0000021A9A5923E0>
max_ep_steps:
  desc: null
  value:
  - 100
max_timesteps:
  desc: null
  value: 300000
n_envs:
  desc: null
  value: 1
n_substeps:
  desc: null
  value: 20
num_timesteps:
  desc: null
  value: 0
obs_type:
  desc: null
  value:
  - ee
  - js
observation_space:
  desc: null
  value: 'Dict(''achieved_goal'': Box(-10.0, 10.0, (3,), float32), ''desired_goal'':
    Box(-10.0, 10.0, (3,), float32), ''observation'': Box(-10.0, 10.0, (38,), float32))'
optimize_memory_usage:
  desc: null
  value: 'False'
policy:
  desc: null
  value: "MultiInputPolicy(\n  (actor): Actor(\n    (features_extractor): CombinedExtractor(\n\
    \      (extractors): ModuleDict(\n        (achieved_goal): Flatten(start_dim=1,\
    \ end_dim=-1)\n        (desired_goal): Flatten(start_dim=1, end_dim=-1)\n    \
    \    (observation): Flatten(start_dim=1, end_dim=-1)\n      )\n    )\n    (latent_pi):\
    \ Sequential(\n      (0): Linear(in_features=44, out_features=400, bias=True)\n\
    \      (1): ReLU()\n      (2): Linear(in_features=400, out_features=300, bias=True)\n\
    \      (3): ReLU()\n    )\n    (mu): Sequential(\n      (0): Linear(in_features=300,\
    \ out_features=7, bias=True)\n      (1): Hardtanh(min_val=-2.0, max_val=2.0)\n\
    \    )\n  )\n  (critic): Critic(\n    (features_extractor): CombinedExtractor(\n\
    \      (extractors): ModuleDict(\n        (achieved_goal): Flatten(start_dim=1,\
    \ end_dim=-1)\n        (desired_goal): Flatten(start_dim=1, end_dim=-1)\n    \
    \    (observation): Flatten(start_dim=1, end_dim=-1)\n      )\n    )\n    (qf0):\
    \ Sequential(\n      (0): Linear(in_features=51, out_features=400, bias=True)\n\
    \      (1): ReLU()\n      (2): Linear(in_features=400, out_features=300, bias=True)\n\
    \      (3): ReLU()\n      (4): Linear(in_features=300, out_features=25, bias=True)\n\
    \    )\n    (qf1): Sequential(\n      (0): Linear(in_features=51, out_features=400,\
    \ bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=400, out_features=300,\
    \ bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=300, out_features=25,\
    \ bias=True)\n    )\n  )\n  (critic_target): Critic(\n    (features_extractor):\
    \ CombinedExtractor(\n      (extractors): ModuleDict(\n        (achieved_goal):\
    \ Flatten(start_dim=1, end_dim=-1)\n        (desired_goal): Flatten(start_dim=1,\
    \ end_dim=-1)\n        (observation): Flatten(start_dim=1, end_dim=-1)\n     \
    \ )\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=51, out_features=400,\
    \ bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=400, out_features=300,\
    \ bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=300, out_features=25,\
    \ bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=51,\
    \ out_features=400, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=400,\
    \ out_features=300, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=300,\
    \ out_features=25, bias=True)\n    )\n  )\n)"
policy_class:
  desc: null
  value: <class 'sb3_contrib.tqc.policies.MultiInputPolicy'>
policy_kwargs:
  desc: null
  value: '{''log_std_init'': -3, ''net_arch'': [400, 300], ''use_sde'': True}'
policy_type:
  desc: null
  value: MultiInputPolicy
prior_steps:
  desc: null
  value: 0
randomize_robot_pose:
  desc: null
  value: false
render:
  desc: null
  value: true
replay_buffer:
  desc: null
  value: stable_baselines3.her.her_replay_buffer.HerReplayBuffer
replay_buffer_class:
  desc: null
  value: <class 'stable_baselines3.her.her_replay_buffer.HerReplayBuffer'>
replay_buffer_kwargs:
  desc: null
  value: '{}'
reward_thresholds:
  desc: null
  value:
  - -1
reward_type:
  desc: null
  value: sparse
sde_sample_freq:
  desc: null
  value: -1
seed:
  desc: null
  value: 1
show_debug_labels:
  desc: null
  value: true
show_goal_space:
  desc: null
  value: true
stages:
  desc: null
  value:
  - wang_4
start_time:
  desc: null
  value: 1683527425532758600
target_entropy:
  desc: null
  value: '-7.0'
target_update_interval:
  desc: null
  value: 1
tau:
  desc: null
  value: 0.02
tensorboard_log:
  desc: null
  value: runs/6u17mck3
top_quantiles_to_drop_per_net:
  desc: null
  value: 2
train_freq:
  desc: null
  value: 'TrainFreq(frequency=8, unit=<TrainFrequencyUnit.STEP: ''step''>)'
use_sde:
  desc: null
  value: 'True'
use_sde_at_warmup:
  desc: null
  value: 'False'
verbose:
  desc: null
  value: 1
