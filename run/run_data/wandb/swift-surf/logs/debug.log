2023-05-08 08:30:21,817 INFO    MainThread:10060 [wandb_setup.py:_flush():76] Configure stats pid to 10060
2023-05-08 08:30:21,818 INFO    MainThread:10060 [wandb_setup.py:_flush():76] Loading settings from C:\Users\eclip\.config\wandb\settings
2023-05-08 08:30:21,818 INFO    MainThread:10060 [wandb_setup.py:_flush():76] Loading settings from C:\Users\eclip\GitHub\panda-gym\run\wandb\settings
2023-05-08 08:30:21,818 WARNING MainThread:10060 [wandb_setup.py:_flush():76] Unknown environment variable: WANDB_KEY
2023-05-08 08:30:21,818 INFO    MainThread:10060 [wandb_setup.py:_flush():76] Loading settings from environment variables: {}
2023-05-08 08:30:21,818 INFO    MainThread:10060 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'run\\train.py', 'program': 'C:\\Users\\eclip\\GitHub\\panda-gym\\run\\train.py'}
2023-05-08 08:30:21,818 INFO    MainThread:10060 [wandb_setup.py:_flush():76] Applying login settings: {'api_key': '***REDACTED***'}
2023-05-08 08:30:21,818 INFO    MainThread:10060 [wandb_setup.py:_flush():76] Applying login settings: {'api_key': '***REDACTED***'}
2023-05-08 08:30:21,818 INFO    MainThread:10060 [wandb_init.py:_log_setup():506] Logging user logs to run_data\wandb\run-20230508_083021-6u17mck3\logs\debug.log
2023-05-08 08:30:21,819 INFO    MainThread:10060 [wandb_init.py:_log_setup():507] Logging internal logs to run_data\wandb\run-20230508_083021-6u17mck3\logs\debug-internal.log
2023-05-08 08:30:21,821 INFO    MainThread:10060 [wandb_init.py:init():546] calling init triggers
2023-05-08 08:30:21,822 INFO    MainThread:10060 [wandb_init.py:init():552] wandb.init called with sweep_config: {}
config: {'env_name': 'PandaReachAO-v3', 'algorithm': 'TQC', 'reward_type': 'sparse', 'goal_distance_threshold': 0.05, 'max_timesteps': 300000, 'seed': 1, 'render': True, 'n_substeps': 20, 'obs_type': ('ee', 'js'), 'control_type': 'js', 'limiter': 'sim', 'action_limiter': 'clip', 'show_goal_space': True, 'replay_buffer': <class 'stable_baselines3.her.her_replay_buffer.HerReplayBuffer'>, 'policy_type': 'MultiInputPolicy', 'show_debug_labels': True, 'n_envs': 1, 'max_ep_steps': [100], 'eval_freq': 5000, 'stages': ['wang_4'], 'reward_thresholds': [-1], 'joint_obstacle_observation': 'all2', 'learning_starts': 10000, 'prior_steps': 0, 'randomize_robot_pose': False, 'hyperparams': {'learning_rate': 0.00073, 'gamma': 0.98, 'tau': 0.02, 'buffer_size': 300000, 'gradient_steps': 8, 'train_freq': 8, 'ent_coef': 'auto', 'use_sde': True, 'policy_kwargs': {'log_std_init': -3, 'net_arch': [400, 300]}}}
2023-05-08 08:30:21,824 INFO    MainThread:10060 [wandb_init.py:init():602] starting backend
2023-05-08 08:30:21,824 INFO    MainThread:10060 [wandb_init.py:init():606] setting up manager
2023-05-08 08:30:21,835 INFO    MainThread:10060 [backend.py:_multiprocessing_setup():106] multiprocessing start_methods=spawn, using: spawn
2023-05-08 08:30:21,844 INFO    MainThread:10060 [wandb_init.py:init():613] backend started and connected
2023-05-08 08:30:21,926 INFO    MainThread:10060 [wandb_init.py:init():701] updated telemetry
2023-05-08 08:30:22,167 INFO    MainThread:10060 [wandb_init.py:init():741] communicating run to backend with 60.0 second timeout
2023-05-08 08:30:22,887 INFO    MainThread:10060 [wandb_run.py:_on_init():2133] communicating current version
2023-05-08 08:30:23,131 INFO    MainThread:10060 [wandb_run.py:_on_init():2142] got version response upgrade_message: "wandb version 0.15.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2023-05-08 08:30:23,132 INFO    MainThread:10060 [wandb_init.py:init():789] starting run threads in backend
2023-05-08 08:30:23,309 INFO    MainThread:10060 [wandb_run.py:_console_start():2114] atexit reg
2023-05-08 08:30:23,312 INFO    MainThread:10060 [wandb_run.py:_redirect():1969] redirect: 3
2023-05-08 08:30:23,312 INFO    MainThread:10060 [wandb_run.py:_redirect():2034] Wrapping output streams.
2023-05-08 08:30:23,312 INFO    MainThread:10060 [wandb_run.py:_redirect():2059] Redirects installed.
2023-05-08 08:30:23,313 INFO    MainThread:10060 [wandb_init.py:init():831] run started, returning control to user process
2023-05-08 08:30:25,535 INFO    MainThread:10060 [wandb_run.py:_tensorboard_callback():1396] tensorboard callback: runs/6u17mck3\TQC_1, True
2023-05-08 08:30:25,750 INFO    MainThread:10060 [wandb_run.py:_config_callback():1251] config_cb None None {'algo': 'TQC', 'policy_class': "<class 'sb3_contrib.tqc.policies.MultiInputPolicy'>", 'device': 'cpu', 'verbose': 1, 'policy_kwargs': "{'log_std_init': -3, 'net_arch': [400, 300], 'use_sde': True}", 'num_timesteps': 0, '_total_timesteps': 300000, '_num_timesteps_at_start': 0, 'action_noise': 'None', 'start_time': 1683527425532758600, 'learning_rate': 0.00073, 'tensorboard_log': 'runs/6u17mck3', '_last_obs': "OrderedDict([('achieved_goal', array([[ 4.8420656e-01, -5.1861488e-12,  4.1103777e-01]], dtype=float32)), ('desired_goal', array([[-0.36585835,  0.51871973,  0.26197878]], dtype=float32)), ('observation', array([[ 4.8420656e-01, -5.1861488e-12,  4.1103777e-01,  0.0000000e+00,\n        -0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -3.0000001e-01,\n         0.0000000e+00, -2.2000000e+00,  0.0000000e+00,  2.0000000e+00,\n         7.8539819e-01,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         2.8076324e-01,  6.2981808e-01,  2.8484035e-01,  4.9770206e-01,\n         2.8617457e-01,  5.5290097e-01,  3.1935012e-01,  6.6397059e-01,\n         3.5679859e-01,  2.0234461e-01,  4.6441135e-01,  1.8427321e-01,\n         5.5766946e-01,  2.7388373e-01,  5.7155538e-01,  5.0419664e-01,\n         6.2862748e-01,  5.5986226e-01]], dtype=float32))])", '_last_episode_starts': '[ True]', '_last_original_obs': 'None', '_episode_num': 0, 'use_sde': 'True', 'sde_sample_freq': -1, '_current_progress_remaining': 1.0, '_stats_window_size': 100, 'ep_info_buffer': 'deque([], maxlen=100)', 'ep_success_buffer': 'deque([], maxlen=100)', '_n_updates': 0, '_custom_logger': 'False', 'env': '<stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000021A7D1F8710>', '_vec_normalize_env': 'None', 'observation_space': "Dict('achieved_goal': Box(-10.0, 10.0, (3,), float32), 'desired_goal': Box(-10.0, 10.0, (3,), float32), 'observation': Box(-10.0, 10.0, (38,), float32))", 'action_space': 'Box(-1.0, 1.0, (7,), float32)', 'buffer_size': 300000, 'batch_size': 256, 'tau': 0.02, 'gamma': 0.98, 'gradient_steps': 8, 'optimize_memory_usage': 'False', 'replay_buffer_class': "<class 'stable_baselines3.her.her_replay_buffer.HerReplayBuffer'>", 'replay_buffer_kwargs': '{}', '_episode_storage': 'None', 'train_freq': "TrainFreq(frequency=8, unit=<TrainFrequencyUnit.STEP: 'step'>)", 'use_sde_at_warmup': 'False', 'target_entropy': '-7.0', 'log_ent_coef': 'tensor([0.], requires_grad=True)', 'ent_coef': 'auto', 'target_update_interval': 1, 'ent_coef_optimizer': 'Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.00073\n    maximize: False\n    weight_decay: 0\n)', 'top_quantiles_to_drop_per_net': 2, 'lr_schedule': '<function constant_fn.<locals>.func at 0x0000021A9A5923E0>', 'policy': 'MultiInputPolicy(\n  (actor): Actor(\n    (features_extractor): CombinedExtractor(\n      (extractors): ModuleDict(\n        (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n        (desired_goal): Flatten(start_dim=1, end_dim=-1)\n        (observation): Flatten(start_dim=1, end_dim=-1)\n      )\n    )\n    (latent_pi): Sequential(\n      (0): Linear(in_features=44, out_features=400, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=400, out_features=300, bias=True)\n      (3): ReLU()\n    )\n    (mu): Sequential(\n      (0): Linear(in_features=300, out_features=7, bias=True)\n      (1): Hardtanh(min_val=-2.0, max_val=2.0)\n    )\n  )\n  (critic): Critic(\n    (features_extractor): CombinedExtractor(\n      (extractors): ModuleDict(\n        (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n        (desired_goal): Flatten(start_dim=1, end_dim=-1)\n        (observation): Flatten(start_dim=1, end_dim=-1)\n      )\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=51, out_features=400, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=400, out_features=300, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=300, out_features=25, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=51, out_features=400, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=400, out_features=300, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=300, out_features=25, bias=True)\n    )\n  )\n  (critic_target): Critic(\n    (features_extractor): CombinedExtractor(\n      (extractors): ModuleDict(\n        (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n        (desired_goal): Flatten(start_dim=1, end_dim=-1)\n        (observation): Flatten(start_dim=1, end_dim=-1)\n      )\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=51, out_features=400, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=400, out_features=300, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=300, out_features=25, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=51, out_features=400, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=400, out_features=300, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=300, out_features=25, bias=True)\n    )\n  )\n)', 'actor': 'Actor(\n  (features_extractor): CombinedExtractor(\n    (extractors): ModuleDict(\n      (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n      (desired_goal): Flatten(start_dim=1, end_dim=-1)\n      (observation): Flatten(start_dim=1, end_dim=-1)\n    )\n  )\n  (latent_pi): Sequential(\n    (0): Linear(in_features=44, out_features=400, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=400, out_features=300, bias=True)\n    (3): ReLU()\n  )\n  (mu): Sequential(\n    (0): Linear(in_features=300, out_features=7, bias=True)\n    (1): Hardtanh(min_val=-2.0, max_val=2.0)\n  )\n)', 'critic': 'Critic(\n  (features_extractor): CombinedExtractor(\n    (extractors): ModuleDict(\n      (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n      (desired_goal): Flatten(start_dim=1, end_dim=-1)\n      (observation): Flatten(start_dim=1, end_dim=-1)\n    )\n  )\n  (qf0): Sequential(\n    (0): Linear(in_features=51, out_features=400, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=400, out_features=300, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=300, out_features=25, bias=True)\n  )\n  (qf1): Sequential(\n    (0): Linear(in_features=51, out_features=400, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=400, out_features=300, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=300, out_features=25, bias=True)\n  )\n)', 'critic_target': 'Critic(\n  (features_extractor): CombinedExtractor(\n    (extractors): ModuleDict(\n      (achieved_goal): Flatten(start_dim=1, end_dim=-1)\n      (desired_goal): Flatten(start_dim=1, end_dim=-1)\n      (observation): Flatten(start_dim=1, end_dim=-1)\n    )\n  )\n  (qf0): Sequential(\n    (0): Linear(in_features=51, out_features=400, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=400, out_features=300, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=300, out_features=25, bias=True)\n  )\n  (qf1): Sequential(\n    (0): Linear(in_features=51, out_features=400, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=400, out_features=300, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=300, out_features=25, bias=True)\n  )\n)', 'batch_norm_stats': '[]', 'batch_norm_stats_target': '[]', '_logger': '<stable_baselines3.common.logger.Logger object at 0x0000021AA084A250>'}
2023-05-08 12:56:53,270 WARNING MsgRouterThr:10060 [router.py:message_loop():77] message_loop has been closed
